---
title: "615 Final Project"
author: "Yuetian Sun U55385536"
date: "December 15, 2017"
output: pdf_document
---
##Introduction

As we all know that President Trump has published a policy about reformation of tax these days. This policy has significant effect on thousands of people's life for example graduate student may have much heavier burdern about their tuition fee. This project aims to extract useful information when people twittering about tax finding out what people atitudes are and what they are most concerned about. This project is devided into several parts. Firstly, to have first impression of data, we plot the density graph and points on map about which area's people are more likely to twitter about tax. Then we conduct Emoji Analysis on these twitters trying to find people's opinion by analyzing emojis. Next we take a further step in the text part of twitters. We conduct Topic Modeling Analysis, Sentiment Analysis and make a perdict of American people thought when talking about tax. As a result of these, we can make an assumption of people satisfaction this policy.

\textbf{Key words:} Twitter, Emoji Analysis, Topic Modeling, Sentiment Analysis

```{r,out.width="30%", echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#insert some image related to the project
myimages<-list.files("./image", pattern = ".jpg", full.names = TRUE)
myimages1 <- list.files("./image",pattern = '.png',full.names = TRUE)
myimage <- c(myimages,myimages1)
knitr :: include_graphics(myimage)
```

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
### IF YOU WANT TO REPRODUCE THE REPORT JUST HIT THE KNIT BUTTON
### BUT IF YOU WANT TO USE THE CODE TO CREAT A NEW DATASET AND GET NEW RESULT
### YOU NEED TO RUN THE COMMENT PART CODE AS WELL

library(twitteR)
library(devtools)
library(reshape)
library(plyr)
library(ggplot2)
library(splitstackshape)
library(stringr)
library(maps)
library(ggmap)

# set up Twitter Authentication

#api_key <- 	"Sj6sj740LrXyuxAtO5SrAG9RJ"
#api_secret <- "gcAo2fhRnJmUbsK8yI9rzzHxu3XDwfy0W8G4uB0kzw65kp1dT4"
#access_token <- "824431253743108096-7DJFRF4EVjdW5UGf0TPQSm5JKQUUHEX"
#access_token_secret <- "lN56R9L9uxxwU7d3EnGmnM0IecO8xtkueoLmqSnkORdXJ"

#setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
```
##1 Exploratory Analysis of Data
We randomly select 47474 of twitters with "tax"" from 11/24/2018 to 12/04/2018. Now let's take a first look of the data. We would like to figure out the general situation about twittering with "tax" across the US. Thus, we make a point plot and density plot on map to have a first impression.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#generate geocode and corresponding twitter
#radius from randomly chosen location

#radius='100mi'
#lat<-runif(n=100,min=24.446667, max=49.384472)
#long<-runif(n=100,min=-124.733056, max=-66.949778)

#generate data fram with random longitude, latitude and chosen radius

#coordinates<-as.data.frame(cbind(lat,long,radius))
#coordinates$lat<-lat
#coordinates$long<-long

#create a string of the lat, long, and radius for entry into searchTwitter()

#for(i in 1:length(coordinates$lat)){
#coordinates$search.twitter.entry[i]<-toString(c(coordinates$lat[i],
#coordinates$long[i],radius))
#}

# take out spaces in the string

#coordinates$search.twitter.entry<-gsub(" ","", coordinates$search.twitter.entry ,
#fixed=TRUE)
#df <- NULL

#Search twitter at each location, check how many tweets and put into dataframe

#for(i in 1:length(coordinates$lat)){
  #tw_raw <- searchTwitter(searchString="tax",n=1000,geocode=coordinates$search.twitter.entry[i])
  #coordinates$number.of.tweets[i]<-length(tw_raw)
  #if (length(tw_raw)!= 0)  
  #{
    #df_raw <- as.data.frame(twListToDF(tw_raw))
    #df <- rbind(df,df_raw)
  #}
#}

#making the US map
#plot all points on the map

#coordinates_tidy <- coordinates[which(coordinates$number.of.tweets>200),]

#save the map dataset

#setwd("/Users/sunyuetian/Desktop/testrepo/615 final project")
#write.csv(coordinates_tidy, paste0('tweets.map_', ht, '.csv'), row.names = FALSE)

coordinates_tidy <-read.csv('tweets.map_tax.csv')

#plot the us map and points to show how many tweets in different area
#all_states <- map_data("state")
#p <- ggplot()
#p <- p + geom_polygon( data=all_states, aes(x=long, y=lat, group = group),colour="grey",fill=NA )
#p <- p + geom_point( data=coordinates_tidy, aes(x=long, y=lat,size=number.of.tweets
#  ), color="blue", alpha = 1/2) + scale_size(name="# of tweets")+ggtitle("Point plot for #keyword 'tax' from domestic twitter users")
#saveRDS(p, "usmap1.rds")

#show the point plot
usmap1 = readRDS("usmap1.rds")
usmap1
```

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE,fig.height=7, fig.width=7}
#plot the density plot
#map<-get_map(location='united states', zoom=4, maptype = "terrain",
#             source='google',color='color')
#saveRDS(map, "usmap2.rds")
map = readRDS("usmap2.rds")
ggmap(map)+ggtitle("Geolocaion density plot for keyword 'tax'")+stat_density2d(
aes(x = long, y = lat, alpha = ..level..),
size = 0.5, bins = 3, data = coordinates_tidy,
geom = "polygon",colour = "black")
```

As we can see the plot above, the point plot and the density show that people in the east coast and north middle part of America have more twitters about "tax", which means in some sense they are more concerned about tax issue. We can guess that may be the east coast has lots of schools and has large population. The reformation of tax will have significant effect on this area. And also people live in the north middle parts of America don't have very high salary on average and the reduction of individual income tax can help them have better life. 

##2 Emoji Analysis

Now we want to take a deep step on people's attitude when twittering "tax". Let us take a looking of what emojis people would like to twitter about "tax". First of all, we need to clean the dataset and extract the emojis (emojis code) from the tweets we selected. Then by pairing the emojis code with emojis dictionary and emojis picture, we can get the original emojis. Then we calculate the frequency of these emojis and figue out which one people are more likely to use when twittering about "tax".

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
# get the tax tweet data and do data cleaning

#ht <- 'tax'
#df$hashtag <- ht
#df$created <- as.POSIXlt(df$created)
#df$text <- iconv(df$text, 'latin1', 'ASCII', 'byte')
#df$url <- paste0('https://twitter.com/', df$screenName, '/status/', df$id)
#df <- rename(df, c(retweetCount = 'retweets'))
#df.a <- subset(df, select = c(text, created, url, latitude, longitude, retweets, hashtag))

#save the cleaned dataset

#setwd("/Users/sunyuetian/Desktop/testrepo/615 final project")
#write.csv(df.a, paste0('tweets.cleaned_', ht, '.csv'), row.names = FALSE)
#tweets <- df; tweets$z <- 1
#tweets$created <- as.POSIXlt(tweets$created)

#save and read twitter dataset for text analysis
#write.csv(tweets, paste0('tweets.text', ht, '.csv'), row.names = FALSE)
tweets <- read.csv("tweets.text_tax.csv")
```

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#emoji analysis
fnames <- c(
  'tweets.cleaned_tax_saved'
)
fnames <- paste0(fnames, '.csv')
df <- do.call(rbind.fill, lapply(fnames, read.csv));
df$username <- substr(substr(df$url, 21, nchar(as.character(df$url))), 1, nchar(substr(df$url, 21, nchar(as.character(df$url))))-26);
tweets.full <- df; tweets.full$X <- NULL; tweets.full$z <- 1; 
#### sanity checking
tweets.full$created <- as.POSIXlt(tweets.full$created)
## dedupe dataset by url
tweets.dupes <- tweets.full[duplicated(tweets.full$url), ]
tweets <- tweets.full[!duplicated(tweets.full$url), ]
tweets <- arrange(tweets, url)
row.names(tweets) <- NULL
tweets$tweetid <- as.numeric(row.names(tweets))
tweets.final <- tweets;

#READ IN EMOJI DICTIONARIES
emdict.la <- read.csv('emoticon_conversion_noGraphic.csv', header = F); 
emdict.la <- emdict.la[-1, ]; row.names(emdict.la) <- NULL; names(emdict.la) <- c('unicode', 'bytes', 'name'); emdict.la$emojiid <- row.names(emdict.la);
emdict.jpb <- read.csv('emDict.csv', header = F) 
emdict.jpb <- emdict.jpb[-1, ]; row.names(emdict.jpb) <- NULL; names(emdict.jpb) <- c('name', 'bytes', 'rencoding'); emdict.jpb$name <- tolower(emdict.jpb$name);
emdict.jpb$bytes <- NULL;
## merge dictionaries
emojis <- merge(emdict.la, emdict.jpb, by = 'name');  emojis$emojiid <- as.numeric(emojis$emojiid); emojis <- arrange(emojis, emojiid);

#FIND TOP EMOJIS FOR A GIVEN SUBSET OF THE DATA
tweets <- tweets.final;
## create full tweets by emojis matrix
df.s <- matrix(NA, nrow = nrow(tweets), ncol = ncol(emojis)); 
df.s <- sapply(emojis$rencoding, regexpr, tweets$text, ignore.case = T, useBytes = T)
df.s <- sapply(emojis$rencoding, regexpr, tweets$text, ignore.case = T, useBytes = T)
rownames(df.s) <- 1:nrow(df.s); colnames(df.s) <- 1:ncol(df.s); df.t <- data.frame(df.s); df.t$tweetid <- tweets$tweetid;
# merge in hashtag data from original tweets dataset
df.a <- subset(tweets, select = c(tweetid, hashtag)); 
df.u <- merge(df.t, df.a, by = 'tweetid'); df.u$z <- 1; df.u <- arrange(df.u, tweetid); 
tweets.emojis.matrix <- df.u;
## create emoji count dataset
df <- subset(tweets.emojis.matrix)[, c(2:843)]; count <- colSums(df > -1);
emojis.m <- cbind(count, emojis); emojis.m <- arrange(emojis.m, desc(count));
emojis.count <- subset(emojis.m, count > 1)
emojis.count$dens <- round(1000 * (emojis.count$count / nrow(tweets)), 1)
emojis.count$dens.sm <- (emojis.count$count + 1) / (nrow(tweets) + 1);
emojis.count$rank <- as.numeric(row.names(emojis.count));
emojis.count.p <- subset(emojis.count, select = c(name, dens, count, rank));
# print summary stats
num.tweets <- nrow(tweets); df.t <- rowSums(tweets.emojis.matrix[, c(2:843)] > -1); num.tweets.with.emojis <- length(df.t[df.t > 0]); num.emojis <- sum(emojis.count$count);

##### MAKE BAR CHART OF TOP EMOJIS IN NEW DATASET
df.plot <- subset(emojis.count.p, rank <= 10)
knitr::kable(df.plot, caption = "Most frequently used emojis")
#saveRDS(emojis.count.p, "emojis_count_p.rds")
xlab <- 'Rank'
ylab <- 'Overall Frequency (per 1,000 Tweets)'
df.plot <- arrange(df.plot, name)
imgs <- lapply( paste0('ios_9_3_emoji_files/',df.plot$name, '.png'), png::readPNG)
g <- lapply(imgs, grid::rasterGrob)
k <- 0.20 * (10/nrow(df.plot)) * max(df.plot$dens)
df.plot$xsize <- k
df.plot$ysize <- k
df.plot <- arrange(df.plot, name);
df.plot$xsize <- k
    df.plot$ysize <- k
    df.plot <- arrange(df.plot, name);
#emojiplot1 <- 
ggplot(data = df.plot, aes(x = rank, y = dens)) +
      geom_bar(stat = 'identity', fill = 'dodgerblue4') +
      xlab(xlab) + ylab(ylab) +
      mapply(function(x, y, i) {
        annotation_custom(g[[i]], xmin = x-0.5*df.plot$xsize[i], xmax = x+0.5*df.plot$xsize[i], ymin = y-0.5*df.plot$ysize[i], ymax = y+0.5*df.plot$ysize[i])},
        df.plot$rank, df.plot$dens, seq_len(nrow(df.plot))) +
      scale_x_continuous(expand = c(0, 0), breaks = seq(1, nrow(df.plot), 1), labels = seq(1, nrow(df.plot), 1)) +
      scale_y_continuous(expand = c(0, 0), limits = c(0, 1.10 * max(df.plot$dens))) +
      theme(panel.grid.minor.y = element_blank(),
            axis.title.x = element_text(size = 10), axis.title.y = element_text(size = 14), 
            axis.text.x  = element_text(size = 8, colour = 'black'), axis.text.y  = element_text(size = 8, colour = 'black'))+ggtitle("Plot of emoji Frequency")
#saveRDS(emojiplot1,"emojiplot1.rds") 
#saveRDS(df, "df.rds")
```

As we can see from the plot and the table above, the most frequently uesd emoji when talking about tax is money bag. This make sense because tax directly related to money and this should be the thing people most concerned about. The third emoji is chart with downwards trend. We can assume that this is a negative one and people using this to express their life going down or it may be a positive one used to express the tax reduction. Also the fourth to sixth shows people concern about hospital fee, tuition fee and many other fee related to tax policy. And the face with tears of joy can some how show the people's atitude about tax reduction.

The next step after visualizing the top emojis in the overall dataset is to compare emoji frequency between two different subsets of the data. To find out more information about it, we can give several pairs of important words people may twitter with "tax" and comparing which emoji they would like to use. We take keywords "trump" and "cut" as an example.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#CREATE MASTER DATASET OF ORIGINAL TWEETS appended with array of emojis
#EMOJIS: create reduced tweets+emojis matrix
df.s <- data.frame(matrix(NA, nrow = nrow(tweets), ncol = 2))
names(df.s) <- c('tweetid', 'emoji.ids')
df.s$tweetid <- 1:nrow(tweets)
df.s$emoji.ids <- apply(tweets.emojis.matrix[, c(2:843)], 1, function(x) paste(which(x > -1), sep = '', collapse = ', '))
df.s$num.emojis <- sapply(df.s$emoji.ids, function(x) length(unlist(strsplit(x, ', '))))
    df.s.emojis <- subset(df.s, num.emojis > 0)
    df.s.nonemojis <- subset(df.s, num.emojis == 0)
    df.s.nonemojis$emoji.names <- ''
# convert to long, only for nonzero entries
df.l <- cSplit(df.s.emojis, splitCols = 'emoji.ids', sep = ', ', direction = 'long')
map <- subset(emojis, select = c(emojiid, name))
map$emojiid <- as.numeric(map$emojiid)
df.m <- merge(df.l, map, by.x = 'emoji.ids', by.y = 'emojiid')
df.m <- arrange(df.m, tweetid)
df.m <- rename(df.m, c(name = 'emoji.name'))
tweets.emojis.long <- subset(df.m, select = c(tweetid, emoji.name))
df.n <- aggregate(emoji.name ~ tweetid, paste, collapse = ', ', data = df.m)
## merge back with original tweets dataset
df.f <- merge(df.s.emojis, df.n, by = 'tweetid')
df.f <- rename(df.f, c(emoji.name = 'emoji.names'))
df.g <- rbind(df.f, df.s.nonemojis)
df.g <- arrange(df.g, tweetid)
df.h <- merge(tweets, df.g, by = 'tweetid', all.x = TRUE)
df.h$emoji.ids <- NULL
df.h$tweetid <- as.numeric(df.h$tweetid)
df.h <- arrange(df.h, tweetid);
tweets.emojis <- df.h;
#saveRDS(tweets.emojis, "tweets_emojis.rds")

#MAKE TWO WAY PLOT FOR A SET OF MUTUALLY EXCLUSIVE SUBSETS OF THE DATA
df.1 <- subset(tweets.emojis, grepl(paste(c('cut'), collapse = '|'), tolower(tweets.emojis$text)));
df.2 <- subset(tweets.emojis, grepl(paste(c('trump'), collapse = '|'), tolower(tweets.emojis$text)));
# dataset 1
df.a <- subset(subset(df.1, emoji.names != ''), select = c(tweetid, emoji.names)) 
df.a$emoji.names <- as.character(df.a$emoji.names)
df.b <- data.frame(table(unlist(strsplit(df.a$emoji.names, ','))))
names(df.b) <- c('var', 'freq')
df.b$var <- trimws(df.b$var, 'both')
df.b <- subset(df.b, var != '')
df.c <- aggregate(freq ~ var, data = df.b, function(x) sum(x))
df.c <- df.c[with(df.c, order(-freq)), ]
row.names(df.c) <- NULL;
df.d <- subset(df.c, freq > 1)
df.d$dens <- round(1000 * (df.d$freq / nrow(df)), 1)
df.d$dens.sm <- (df.d$freq + 1) / (nrow(df) + 1)
df.d$rank <- as.numeric(row.names(df.d))
df.d <- rename(df.d, c(var = 'name'))
df.e <- subset(df.d, select = c(name, dens, dens.sm, freq, rank))
df.e$ht <- as.character(arrange(data.frame(table(tolower(unlist(str_extract_all(df.1$text, '#\\w+'))))), -Freq)$Var1[1]);
emojis.count.1 <- df.e
# dataset 2
df.a <- subset(subset(df.2, emoji.names != ''), select = c(tweetid, emoji.names)) 
df.a$emoji.names <- as.character(df.a$emoji.names)
df.b <- data.frame(table(unlist(strsplit(df.a$emoji.names, ','))))
names(df.b) <- c('var', 'freq')
df.b$var <- trimws(df.b$var, 'both')
df.b <- subset(df.b, var != '')
df.c <- aggregate(freq ~ var, data = df.b, function(x) sum(x))
df.c <- df.c[with(df.c, order(-freq)), ]
row.names(df.c) <- NULL
df.d <- subset(df.c, freq > 1)
df.d$dens <- round(1000 * (df.d$freq / nrow(df)), 1)
df.d$dens.sm <- (df.d$freq + 1) / (nrow(df) + 1)
df.d$rank <- as.numeric(row.names(df.d))
df.d <- rename(df.d, c(var = 'name'))
df.e <- subset(df.d, select = c(name, dens, dens.sm, freq, rank))
df.e$ht <- as.character(arrange(data.frame(table(tolower(unlist(str_extract_all(df.2$text, '#\\w+'))))), -Freq)$Var1[1])
emojis.count.2 <- df.e
# combine datasets and create final dataset
names(emojis.count.1)[-1] <- paste0(names(emojis.count.1)[-1], '.1'); names(emojis.count.2)[-1] <- paste0(names(emojis.count.2)[-1], '.2'); 
df.a <- merge(emojis.count.1, emojis.count.2, by = 'name', all.x = TRUE, all.y = TRUE);
df.a[, c(2:4, 6:8)][is.na(df.a[, c(2:4, 6:8)])] <- 0; df.a <- df.a[with (df.a, order(-dens.1)), ];
df.a$index <- ifelse(df.a$dens.1 > 0 & df.a$dens.2 > 0 & (df.a$dens.1 > df.a$dens.2), round(100 * ((df.a$dens.1 / df.a$dens.2) - 1), 0),
                     ifelse(df.a$dens.1 > 0 & df.a$dens.2 > 0 & (df.a$dens.2 > df.a$dens.1), -1 * round(100 * ((df.a$dens.2 / df.a$dens.1) - 1), 0), NA));
df.a$logor <- log(df.a$dens.sm.1 / df.a$dens.sm.2);
df.a$dens.mean <- 0.5 * (df.a$dens.1 + df.a$dens.2);
k <- 50; df.b <- subset(df.a, (rank.1 <= k | rank.2 <= k) & 
                          (freq.1 >= 5 | freq.2 >= 5) & 
                          (freq.1 > 0 & freq.2 > 0) & dens.mean > 0)
df.c <- subset(df.b, select = c(name, dens.1, dens.2, freq.1, freq.2, dens.mean, round(logor, 2)));
df.c <- df.c[with(df.c, order(-logor)), ]; row.names(df.c) <- NULL;
table1 <- df.c
table1$name[8] <- "regional indicator symbol letter u/s"
table1$name[5] <- "regional indicator symbol letter u/s"
library(knitr)
kable(table1, caption="Table of emoji comparison")
emojis.comp.p <- df.c
#saveRDS(emojis.comp.p, "emojis_comp_p.rds")

#PLOT TOP EMOJIS SCATTERPLOT: FREQ VS VALENCE  
#read in custom emojis
df.t <- arrange(emojis.comp.p, name);
imgs <- lapply(paste0('ios_9_3_emoji_files/', df.t$name, '.png'), png::readPNG)
g <- lapply(imgs, grid::rasterGrob);
#make plot  
df.t <- arrange(emojis.comp.p, logor)
xlab <- paste0('Emoji Valence: Log Odds Ratio (', paste0(unique(emojis.count.2$ht), ' <--> ', unique(emojis.count.1$ht), ')'));
ylab <- 'Overall Frequency (Per 1,000 Tweets)'
k <- 8 # size parameter for median element
xsize <- (k/100) * (max(df.t$logor) - min(df.t$logor)); ysize <- (k/100) * (max(df.t$dens.mean) - min(df.t$dens.mean));
df.t$xsize <- xsize; df.t$ysize <- ysize;
df.t$dens.m <- ifelse(df.t$dens.mean > median(df.t$dens.mean), round(sqrt((df.t$dens.mean / min(df.t$dens.mean))), 2), 1);
df.t$xsize <- df.t$dens.m * df.t$xsize; df.t$ysize <- df.t$dens.m * df.t$ysize;
df.t <- arrange(df.t, name);
#emojiplot2 <- 
ggplot(df.t, aes(jitter(logor), dens.mean)) +
  xlab(xlab) + ylab(ylab) +
  mapply(function(x, y, i) {
    annotation_custom(g[[i]], xmin = x-0.5*df.t$xsize[i], xmax = x+0.5*df.t$xsize[i], 
                      ymin = y-0.5*df.t$ysize[i], ymax = y+0.5*df.t$ysize[i])},
    jitter(df.t$logor), df.t$dens.mean, seq_len(nrow(df.t))) +
  scale_x_continuous(limits = c(1.15 * min(df.t$logor), 1.15 * max(df.t$logor))) +
  scale_y_continuous(limits = c(0, 1.20 * max(df.t$dens.mean))) +
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 10), axis.title.y = element_text(size = 10), 
        axis.text.x  = element_text(size = 8, colour = 'black'), axis.text.y  = element_text(size = 8, colour = 'black'))+ggtitle("Plot of emoji comparison")
#saveRDS(emojiplot2, "emojiplot2.rds")
```

There are multiple pairs of words you can choose for these graph. If you want to see the results of all these words, please use the shiny.app. Here let us interpret "trump" and "cut" example. As we can see above, when people mention "trump" in twitter with "tax", they tends to use face with tears of joy while people mention "cut" are more likely to use money bag. This result makes sense because cutting tax is directly related to money. Also people who mention "cut" have higher probability of using police cars revolving light emoji. This may use to call people's attention about the reformation of tax. In additon, people who mention "cut" or "trump" are equal likely to use person with folded hands. May be they use it to hope for a good life.

After exploring the emojis, we now focus on the text part of twitters.

##3 Text Analysis

Now let us take a look of what words people would likely to use when twittering with "tax". To make the result intuitively, we make a table and a frequency plot of words showing up more than 1500 times.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#text analysis
library(tidytext)
library(tidyverse)
library(wordcloud)
library(RColorBrewer)
library(tokenizers)
library(tm)
library(NLP)
library(topicmodels)
library(SnowballC)
library(reshape2)

#build the dataset and clean it

#build a corpus
#docs <- Corpus(VectorSource(tweets$text))
#Transform to lower case
#docs <-tm_map(docs,content_transformer(tolower))
#remove url's
#removeURL <- function(x) gsub("http.+ |http.+$","",x)
#docs <- tm_map(docs, content_transformer(removeURL))
#remove user's name
#removename <- function(x) gsub("@\\w+","",x)
#docs <- tm_map(docs, content_transformer(removename))
#remove punctuation
#docs <- tm_map(docs, removePunctuation)
#Strip digits
#docs <- tm_map(docs, removeNumbers)
#get the unique twitter
#docs <- tm_map(docs, content_transformer(unique))
#remove stopwords
#docs <- tm_map(docs, removeWords, stopwords('english'))
#remove whitespace
#docs <- tm_map(docs, stripWhitespace)
#remove Stem document
#docs <- tm_map(docs,stemDocument)
#remove other stopwords
#docs <- tm_map(docs, removeWords, c("tax",'amp','rt','taxes'))
#fix some words
#docs <- tm_map(docs, content_transformer(gsub),
#pattern = 'peopl', replacement = 'people')
#docs <- tm_map(docs, content_transformer(gsub),
#pattern = 'senat', replacement = 'senate')
#docs <- tm_map(docs, content_transformer(gsub),
#pattern = 'middl', replacement = 'middle')
#docs <- tm_map(docs, content_transformer(gsub),
#pattern = 'estat', replacement = 'estate')
```

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
##extract words and make it a dataframe
#term <- unlist(docs)
#text_df <- data_frame(line=1:length(term), text = term)
#tidy_tw <- text_df %>% unnest_tokens(word, text)
#write.csv(tidy_tw, paste0('tweets.texttidy', ht, '.csv'), row.names = FALSE)
tidy_tw <- read.csv("tweets.texttidy_tax.csv")
#get the table of word freq
kable(tidy_tw %>%
  count(word, sort = TRUE) %>%
  filter(n > 1500),  caption = "Table of words frequency more than 1500")

#plot the bar plot of the freq of words
tidy_tw %>%
  count(word, sort = TRUE) %>%
  filter(n > 1500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill=word)) +
  geom_col(alpha=0.8) +
  xlab(NULL)+ylab("Frequency")+
  coord_flip() +ggtitle("Bar plot of words frequency more than 1500")
```

From the bar plot and the table above, We can find that people are concerned about cuting their bill, republican and voting things about tax. These make sense because the reducation tax policy is passed in senate and is comed up by Trump, Repulican Party. And the aim of these policy is to cut the tax and promote American's life.

Now let us continue finding out what topics people would likely to talk about tax.

##3.1 Topic Modeling

To take a further step, we conduct Topic Modeling and analyze what topic people will talk when twitter about tax.

Firstly, we would like to introduce what is Topic Modeling and how it works on twitter. In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both.

Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.

In this report, we focus on using LDA to conduct Topic Model. And we would like to choose Gibbs Sampling. Here is how Topic Modeling works mathematically.

The topic distribution for each twitter is distributed as
$$\theta \sim Dirichlet(\alpha)$$
where $Dirichlet(\alpha)$ denotes the Dirichlet distribution for paramter $\alpha$.

The word distribution on the other hand is also modeled by a Dirichlet distribution, just under a different parameter $\eta$.
$$\phi \sim Dirichlet(\eta)$$
The utmost goal of LDA is to estimate the $\theta$ and $\phi$ which is equivalent to estimate which words are important for which topic and which topics are important for a particular document(twitter), respectively.

For each document(twitter) d, go through each word w (a double for loop). Reassign a new topic to w, where we choose topic t with the probability of word w given topic t * probability of topic t given document(twitter) d, denoted by the following mathematical notations:
$$P(z_{i}=j | z_{-i},w_{i},d_{i})=\frac{C^{WT}_{w_{i}j}+\eta}{\sum_{w=1}^{W}C^{WT}_{wj}+W\eta} * \frac{C^{DT}_{d_{i}j}+\alpha}{\sum_{t=1}^{T}C^{DT}_{d_{i}t}+T\alpha}$$

$P(z_{i}=j)$: The probability that token i is assigned to topic j.

$z_{-i}$ : Represents topic assignments of all other tokens.

$w_i$ : Word (index) of the ith token.

$d_i$ : document(twitter) containing the ith token.

For the right side of the equal sign:

$C^{WT}$ : Word-topic matrix.

$\sum_{w=1}^{W}C^{WT}_{wj}$ : Total number of tokens (words) in each topic.

$C^{DT}$: Document-topic matrix.

$\sum_{w=1}^{W} C^{DT}_{d_{i}t}$ : Total number of tokens (words) in document(twitter) i.

$\eta$ : Parameter that sets the topic distribution for the words, the higher the more spread out the words will be across the specified number of topics (K).

$\alpha$ : Parameter that sets the topic distribution for the documents, the higher the more spread out the tweets will be across the specified number of topics (K).

$W$ : Total number of words in the set of twwets.

$T$ : Number of topics, equivalent of the K we defined earlier.

After we’re done with learning the topics for 1000 iterations, we can use the count matrices to obtain the word-topic distribution and document-topic distribution.

To compute the probability of word given topic:

$$\phi_{ij}=\frac{C^{WT}_{ij}+\eta}{\sum_{k=1}^{W}C^{WT}_{kj}+W\eta}$$

Where $\phi_{ij}$ is the probability of word i for topic j.
$$\theta_{dj}=\frac{C^{DT}_{dj}+\alpha}{\sum_{k=1}^{T}C^{DT}_{dk}+T\alpha}$$
Where $\theta_{dj}$ is the proportion of topic j in document d.

After computing the probability that each document(twitter) belongs to each topic ( same goes for word & topic ) we can use this information to see which topic does each document(twitter) belongs to and the more possible words that are associated with each topic.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
# Now for Topic Modeling
# Get the lengths and make sure we only create a DTM for tweets with some actual content

#doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(docs)))
#dtm <- DocumentTermMatrix(docs[doc.lengths > 0])

#saveRDS(dtm, "dtm.rds")
dtm = readRDS("dtm.rds")
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 10  # Let's start with 10 topics

models <- list(
    CTM = CTM(dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
    VEM = LDA(dtm, k = k, control = list(seed = SEED)),
    VEM_Fixed = LDA(dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
    Gibbs = LDA(dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000, thin = 100, iter = 1000))
)

# Top 10 terms of each topic for each model
m <- lapply(models, terms, 10)
#saveRDS(m, "topic.rds")
d <- as.data.frame(m$Gibbs)
kable(d, caption = 'Ten topics for twitter tax')
```

As we can see from the above table, this is the ten topics people would likely to talk about when twittering "tax". Now let's indicate the first three topics. we can find that topic 1 contain words like "rich", "poor", "middle" and "class". Thus we can make an assumption that people care about how tax affects on different classes. The second topic has "trump", "reform", "thank" and "lie" in it. Thus we can indicate that this may relate to topic about President Trump and reformation of tax. In the third part, we have words "vote", "people", "support" and "follow". Therefore, we can view the third topic as people supporting degree on tax reduction.

##3.2 Sentiment Analysis

Now let us take a look on what people attitude on tax by conducting sentiment analysis. To start with, let us see what words people use to express their emotion. We can choose emotion like joy, angry, disgust. Here we only show joy as an example. If you want to see other emotional words result, please use the shiny.app. There are multiple choices in shiny.app.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#sentiment analysis
#take a look of the joy words in twitter with 'tax'
nrcjoy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")
kable(tidy_tw %>%
  inner_join(nrcjoy) %>%
  count(word, sort = TRUE) %>%
    top_n(10), caption = "Table of top 10 joyful word")
```

As we can see from the above table, the most frequently used joyful words are 'vote', 'pay', 'money' and 'good'. Thus, we can guess that people with positve opinion about 'tax' are likely to talk about vote for the tax policy and the money things.

Now let us analyze people atitude by comparing the positive and negative words.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#plot the num of positive -negative words
twsentiment <- tidy_tw %>%
  inner_join(get_sentiments("bing")) %>%
  count(index = line %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(twsentiment, aes(index, sentiment, fill=index)) +
  geom_col(show.legend = FALSE)+ggtitle("Positive - negative words in every eighty twitter")+ labs(y = "number of positve words minus negative", x = 'every eighty twitter')
```

As we can see from the positive - negative plot, the lines above horizontal line means the positve words are more than negative words in every eighty twitter. This graph show that the case positive words are more than negative happens more than the other case. That is to say, the number of positive words is larger than the number of negative words. Thus we can indicate that the on the average people show positive atitude when twittering about tax. We are now interested in what positive and negative words people most frequently used.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#compare positive-negative
bing_word_counts <- tidy_tw %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
#get the table of positive and negative words
pn <- bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10)
kable(pn, caption = "Table of top 10 positive and negative words")
#plot the freq of top 10 positive and negative words
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()+ggtitle("Frequency of top 10 positive and negative words")
```

As we can see from the plot and table above, the three most frequently used words are all positive. People would likely to use 'reform', 'trump' and 'rich'. We can indicate that people tend to believe the reformation of tax published by Trump can help people become rich and wealthy. But as we can also find that some people use negative words like 'scam', 'poor' and 'break'. Thus, although lots of people support the policy, there are some people think it is a scam and will make people poor.

To have a more intuitive way of the words and its frequency, I make word clouds of top 500 frequently used words and top 200 positive and negative words.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, fig.height=8, fig.width=8, ,fig.align='center'}
#Word Clouds
par(mfrow = c(2, 1))
pal2 <- brewer.pal(8,"Dark2")
tidy_tw %>%
  count(word) %>%
  with(wordcloud(word, n, scale=c(8,.2),min.freq=3,
max.words=500, random.order=FALSE,rot.per=.3, colors = pal2))
#make the world clouds of positive vs. negative words  

tidy_tw %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"), random.order=FALSE,rot.per=.3, max.words = 200)
```

As we see from the two word clouds above, it is very clear tha people concern about the about bill and money staff most and then they also care about the policy itself like the voting things, senate, repiblican, Trump and so on. Besides, people with positive attitude would likely to think about the reform of tax will make them rich while people with negative attitude think the reduction of tax is a scam.

##Conclusion:

In this project, we care about how people think about the tax through twitter. We find that people living in east coast and north middle part show more concern about tax. Besides, by analyzing the emojis, we figue out that people are most concerned about money stuff. They care about hospital fee, tuition fee and many other fee related to tax policy. To narrow down, we use "trump" and "cut" as our second keywords. When people mention "trump" in twitter with "tax", they tends to use face with tears of joy while people mention "cut" are more likely to use money bag. In additon, people who mention "cut" or "trump" are equal likely to use person with folded hands. For the test part, from the bar plots, word clouds and tables, we can find that people are concerned about cuting their bill, republican and voting things about tax. And by conducting Topic Modeling, we find that people would likely to talk about how tax affects on different classes, President Trump and reformation of tax, and supporting degree on tax reduction. Additionaly, by conducting the Sentiment Analysis, we find that people show more postive attitude when twittering about tax and they tend to believe the reformation of tax can bring people good life.

To sum up, when twittering about tax, people are most concerned about the reduction policy and how it affect their money and their life. In general, although some people think this policy is a scam, lots of people do believe this reformation will make them wealthy and they support Trump policy of cuting tax.

##Reference:
[1] URL:	https://deanattali.com/blog/building-shiny-apps-tutorial/#before-we-begin

[2] URL:  http://ethen8181.github.io/machine-learning/clustering_old/topic_model/LDA.html

[3] URL:  https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation